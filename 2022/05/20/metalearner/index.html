<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》 - Gehuiming Zhu&#039;s Site</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Gehuiming Zhu&#039;s Site"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Gehuiming Zhu&#039;s Site"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="“There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any"><meta property="og:type" content="blog"><meta property="og:title" content="An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》"><meta property="og:url" content="https://ghmzhu.github.io/2022/05/20/metalearner/"><meta property="og:site_name" content="Gehuiming Zhu&#039;s Site"><meta property="og:description" content="“There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://ghmzhu.github.io/gallery/covers/metalearner.png"><meta property="article:published_time" content="2022-05-20T02:24:23.000Z"><meta property="article:modified_time" content="2022-07-29T00:31:59.422Z"><meta property="article:author" content="Gehuiming Zhu"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Reading Notes"><meta property="article:tag" content="Causal Inference"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/covers/metalearner.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://ghmzhu.github.io/2022/05/20/metalearner/"},"headline":"An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》","image":["https://ghmzhu.github.io/gallery/covers/metalearner.png"],"datePublished":"2022-05-20T02:24:23.000Z","dateModified":"2022-07-29T00:31:59.422Z","author":{"@type":"Person","name":"Gehuiming Zhu"},"publisher":{"@type":"Organization","name":"Gehuiming Zhu's Site","logo":{"@type":"ImageObject","url":"https://ghmzhu.github.io/img/logo.jpg"}},"description":"“There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any"}</script><link rel="canonical" href="https://ghmzhu.github.io/2022/05/20/metalearner/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.jpg" alt="Gehuiming Zhu&#039;s Site" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/Photography">Photography</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Email me" href="mailto:zhugehuiming@foxmail.com"><i class="fas fa-envelope"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/ghmzhu"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/gallery/covers/metalearner.png" alt="An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-05-20T02:24:23.000Z" title="2022/5/20 10:24:23">2022-05-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-07-29T00:31:59.422Z" title="2022/7/29 08:31:59">2022-07-29</time></span><span class="level-item"><a class="link-muted" href="/categories/Reading-Notes/">Reading Notes</a><span> / </span><a class="link-muted" href="/categories/Reading-Notes/Machine-Learning/">Machine Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile">An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》</h1><div class="content"><p>“There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly.”</p>
<span id="more"></span>
<h2 id="1-Brief-Introduction"><a href="#1-Brief-Introduction" class="headerlink" title="1 Brief Introduction"></a>1 Brief Introduction</h2><h3 id="1-1-Why-Machine-Learning"><a href="#1-1-Why-Machine-Learning" class="headerlink" title="1.1 Why Machine Learning?"></a>1.1 Why Machine Learning?</h3><blockquote>
<p>“同样是拟合一个线性回归模型，传统的社会科学研究者将注意力放在这个模型中特定自变量的系数上，而机器学习的目的则是看这个回归模型多大程度上可以预测因变量的取值。这种关注点上的区分非常重要。因为我们在进行模型拟合时所需要特别关注的问题（例如共线性等）在机器学习的分析范式下便不再是问题。只要有助于提升预测的准确度，我们的模型拟合过程完全可以变得非常有弹性。”</p>
<p>“所谓的‘因果推断的基本问题’，本质上是一个缺失值问题，而为了解决缺失值问题，我们就需要利用已有的资料进行某种意义上的“预测”，这恰恰是机器学习方法的强项所在。”</p>
<p>——胡安宁，《中国社会科学报》</p>
<p>传统回归模型的交互项分析主要有两个问题：交互项不能无限制添加，以及交互项的设定具有主观性。倾向值方法存在模型和系数不确定，以及无法确定具体起异质化作用的混淆变量的问题。</p>
<p>——胡安宁,吴晓刚,陈云松, <a target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2021&amp;filename=SHXJ202101005&amp;uniplatform=NZKPT&amp;v=AlM_7h2Ff4ylnmx_UGCdsCu4wL2jrWRgp4D7EZfZDvNWsybwUMoF6Rjdygqh_jWv">《处理效应异质性分析——机器学习方法带来的机遇与挑战》</a></p>
</blockquote>
<p><strong>Casual Tree:</strong> Athey S, Imbens G. <a target="_blank" rel="noopener" href="https://www.pnas.org/doi/abs/10.1073/pnas.1510489113">Recursive partitioning for heterogeneous causal effects</a>[J]. Proceedings of the National Academy of Sciences, 2016, 113(27): 7353-7360.</p>
<p><strong>Casual Forests:</strong> Wager S, Athey S. <a target="_blank" rel="noopener" href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1319839?journalCode=uasa20">Estimation and inference of heterogeneous treatment effects using random forests</a>[J]. Journal of the American Statistical Association, 2018, 113(523): 1228-1242.</p>
<p><strong>BART:</strong> Chipman H A , Mcculloch G R E . <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/0806.3286.pdf">BART: BAYESIAN ADDITIVE REGRESSION TREES</a>[J]. Annals of Applied Statistics, 2010, 4(1):266-298.</p>
<p><strong>Transfer Learning:</strong> Künzel S R, Stadie B C, Vemuri N, et al. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.07804">Transfer learning for estimating causal effects using neural networks</a>[J]. arXiv preprint arXiv:1808.07804, 2018.</p>
<h3 id="1-2-Framework-and-Definitions"><a href="#1-2-Framework-and-Definitions" class="headerlink" title="1.2 Framework and Definitions"></a>1.2 Framework and Definitions</h3><ul>
<li><p>We use the following representation of $\mathcal{P}$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
X & \sim \Lambda \\\\
W & \sim \operatorname{Bern}(e(X)) \\\\
Y(0) &=\mu_{0}(X)+\varepsilon(0) \\\\
Y(1) &=\mu_{1}(X)+\varepsilon(1)
\end{aligned}</script></li>
<li><p>$\left(Y_{i}(0), Y_{i}(1), X_{i}, W_{i}\right) \sim \mathcal{P}$, where $X_{i} \in \mathbb{R}^{d}$ is a $d$-dimensional covariate or feature vector, $W_{i} \in\{0,1\}$ is the treatment-assignment indicator (to be defined precisely later), $Y_{i}(0) \in \mathbb{R}$ is the potential outcome of unit $i$ when $i$ is assigned to the control group, and $Y_{i}(1)$ is the potential outcome when $i$ is assigned to the treatment group. With this definition, the <code>ATE</code> is defined as</p>
</li>
</ul>
<script type="math/tex; mode=display">
\mathrm{ATE}:=\mathbb{E}[Y(1)-Y(0)]</script><ul>
<li><code>the response under control</code> $\mu_{0}(x):=\mathbb{E}[Y(0) \mid X=x] \quad$ </li>
<li><p><code>the response under treatment</code>$\quad \mu_{1}(x):=\mathbb{E}[Y(1) \mid X=x]$</p>
</li>
<li><p>For a new unit $i$ with covariate vector $x_{i}$, to decide whether to give the unit the treatment, we wish to estimate the <code>ITE</code> of unit $i, D_{i}$, which is defined as</p>
</li>
</ul>
<script type="math/tex; mode=display">
D_{i}=Y_{i}(1)-Y_{i}(0) .</script><ul>
<li><code>the CATE function</code> is defined as</li>
</ul>
<script type="math/tex; mode=display">
\tau(x)=\mathbb{E}[D \mid X=x]=\mathbb{E}[Y(1)-Y(0) \mid X=x],</script><blockquote>
<p><strong>Condition 1:</strong></p>
<script type="math/tex; mode=display">
(\varepsilon(0), \varepsilon(1)) \perp W \mid X .</script><p><strong>Condition 2:</strong> There exists $e_{\min }$ and $e_{\max }$, such that for all $x$ in the support of $X$,</p>
<script type="math/tex; mode=display">
0<e_{\min }<e(x)<e_{\max }<1 .</script></blockquote>
<ul>
<li><p>If we denote our <code>learned estimates</code> as $\hat{\mu}_{0}(x)$ and  $\hat{\mu}_{1}(x)$, then we can form the CATE estimate as the difference between the two</p>
<script type="math/tex; mode=display">
\hat{\tau}(x)=\hat{\mu}_{1}(x)-\hat{\mu}_{0}(x) .</script></li>
<li><p>In this work, we are interested in estimators with a small expected mean squared error (EMSE) for estimating the CATE,</p>
</li>
</ul>
<script type="math/tex; mode=display">
\operatorname{EMSE}(\mathcal{P}, \hat{\tau})=\mathbb{E}\left[(\tau(\mathcal{X})-\hat{\tau}(\mathcal{X}))^{2}\right] .</script><blockquote>
<p><strong>HINT:</strong> We can never observe the heterogeneous treatment effects because we can’t observe the counterfactual, we can only estimate the heterogeneous treatment effects, so we can use the modified MSE formula:</p>
<script type="math/tex; mode=display">
\operatorname{EMSE}(\mathcal{P}, \hat{\tau})=\mathbb{E}\left[(\tau(\mathcal{X})-\hat{\tau}(\mathcal{X}))^{2}-(\tau(\mathcal{X}))^{2}\right] .</script><p><em>(see Susan Atheya and Guido Imbensa,<a target="_blank" rel="noopener" href="https://www.pnas.org/doi/abs/10.1073/pnas.1510489113">《Recursive partitioning for heterogeneous causal effects》</a>)</em></p>
</blockquote>
<h2 id="2-Metaalgorithms"><a href="#2-Metaalgorithms" class="headerlink" title="2 Metaalgorithms"></a>2 Metaalgorithms</h2><h3 id="2-1-Ensemble-Learning"><a href="#2-1-Ensemble-Learning" class="headerlink" title="2.1 Ensemble Learning"></a>2.1 Ensemble Learning</h3><blockquote>
<p>In <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Statistics">statistics</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, <strong>ensemble methods</strong> use multiple learning algorithms to obtain better <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Predictive_inference">predictive performance</a> than could be obtained from any of the constituent learning algorithms alone.</p>
</blockquote>
<p><strong>Bagging (Bootstrap aggregating)</strong></p>
<p>Bagging使用装袋采样来获取数据子集训练基础学习器。通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。例如Random Forest、本文的metalearner等。</p>
<p><strong>Boosting</strong></p>
<p>Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是<strong>利用加权的数据</strong>。在训练的早期对于错分数据给予较大的权重。比较经典的有AdaBoost。</p>
<h3 id="2-2-T-learner"><a href="#2-2-T-learner" class="headerlink" title="2.2 T-learner"></a>2.2 T-learner</h3><p><img src="https://s2.loli.net/2022/05/15/WL1OfU9eawB3MdY.png" alt="截屏2022-05-15 11.38.47" style="zoom:50%;" /></p>
<p><strong>Step1:</strong> 用一个基学习器来拟合对照组的相应函数，$\mu_0(x)=\mathbb{E}[Y(0)|X=x]$。基学习器可以在对照组的样本${(X_i,Y_i)}_{W_i=0}$采用任何监督学习或者回归估计，我们用符号$\hat{\mu_0}$表示。</p>
<p><strong>Step2:</strong> 我们估计treatment的相应函数，$\mu_1(x)=\mathbb{E}[Y(1)|X=x]$,在实验组的数据上进行训练，我们用符号$\hat{\mu_1}$来表示。T-learner可以通过以下公式得出：</p>
<script type="math/tex; mode=display">
\hat{\tau_T}(x)=\hat{\mu_1}(x) -\hat{\mu_0}(x) \\ \tag{3}</script><h3 id="2-3-S-learner"><a href="#2-3-S-learner" class="headerlink" title="2.3 S-learner"></a>2.3 S-learner</h3><p><img src="https://s2.loli.net/2022/05/15/bPpnFaB6kZ3Kutv.png" alt="截屏2022-05-15 11.39.11" style="zoom:50%;" /></p>
<p>treatment是被当做特征使用的。评估公式为$\mu(x,w) := \mathbb{E}[Y^{abs}|X=x,W=w]$,这里可以用任何基学习器，我们用$\hat{\mu}$表示模型的估计量，因此CATE估计量表示为：</p>
<script type="math/tex; mode=display">
\hat{\tau_S}(x) =\hat{\mu}(x,1) -\hat{\mu}(x,0) \tag{4}</script><h3 id="2-4-X-learner"><a href="#2-4-X-learner" class="headerlink" title="2.4 X-learner"></a>2.4 X-learner</h3><p><img src="https://s2.loli.net/2022/05/15/BI71xeMYDiw9rsN.png" alt="截屏2022-05-15 11.39.39" style="zoom:50%;" /></p>
<p><strong>STEP 1:</strong> 用任意的监督学习或者回归算法估计相应函数，用$\hat{\mu_0}$和$\hat{\mu_1}$表示估计量。</p>
<script type="math/tex; mode=display">
\mu_0 = \mathbb{E}[Y(0)|X=x] \tag{5}</script><script type="math/tex; mode=display">
\mu_1 = \mathbb{E}[Y(1)|X=x] \tag{6}</script><p><strong>STEP 2:</strong> 根据对照组的模型来计算实验组中的个人treatment效果，根据实验组的模型来计算实验最中的个人treatment效果。用公式表示为：</p>
<script type="math/tex; mode=display">
\tilde{D_i^1} :=Y_i^1-\hat{\mu_0}(X_i^1) \tag{7}</script><script type="math/tex; mode=display">
\tilde{D_i^0} :=\hat{\mu_1}(X_i^0)-Y_i^0 \tag{8}</script><p>注意到，如果$\hat{\mu_0}=\mu_0$和$\hat{\mu_1}=\mu_1$，则$\tau(x)=\mathbb{E}[\tilde{D^1}|X=x]=\mathbb{E}[\tilde{D^0}|X=x]$</p>
<p>  使用任意的监督学习或者回归算法计算$\tau(x)$有两种方式：一种是利用treatment组训练的模型计算得到的$\hat{\tau_1}(x)$,另一种是利用对照组训练的模型计算得到的$\hat{\tau_0}(x)$.</p>
<p><strong>STEP 3:</strong>  通过阶段2中计算得到的两个估计量进行加权计算CATE估计量：</p>
<div align="center">

<img src="https://s2.loli.net/2022/05/20/ex25CdEUzgY978i.png" alt="截屏2022-05-20 16.06.46" style="zoom:70%;" />

</div>

<p>$g \in [0,1]$是一个权重函数。</p>
<h3 id="2-5-Intuition-Behind-the-Metalearners"><a href="#2-5-Intuition-Behind-the-Metalearners" class="headerlink" title="2.5 Intuition Behind the Metalearners"></a>2.5 Intuition Behind the Metalearners</h3><ul>
<li>Fig. 1A shows the outcome for units in the treatment group (circles) and the outcome of units in the untreated group (crosses). <strong>In this example, the CATE is constant and equal to one.</strong></li>
</ul>
<p><img src="https://s2.loli.net/2022/05/15/ijTDlAw7UFomVye.png" alt="截屏2022-05-15 11.40.18" style="zoom:50%;" /></p>
<ul>
<li>The T-learner would now use estimator $\hat{\tau}_{T}(x)=\hat{\mu}_{1}(x)-\hat{\mu}_{0}(x)$ (Fig. 1C, solid line), which is a relatively complicated function with jumps at 0 and 0.5, while the true  ${\tau}(x)$ is a constant. </li>
<li>For X-learner, If we choose $g(x)=\hat{e}(x)$, an estimator for the propensity score, $\hat{\tau}$ will be very similar to $\hat{\tau}_{1}(x)$, since we have many more observations in the control group; i.e., $\hat{e}(x)$ is small.</li>
</ul>
<p><code>在这个例子中我们选择S-learner很难评估，例如当用RF的基学习器进行训练时，S-learner第一个split可能把97.5%的实验组的样本split出去，造成后续split时缺少实验组的样本。换句话说就是实验组和对照组的样本比例极不均衡时，如果使用S-learner训练时几次split就会把所有的实验组样本使用完。</code></p>
<h2 id="3-Simulation-Results"><a href="#3-Simulation-Results" class="headerlink" title="3 Simulation Results"></a>3 Simulation Results</h2><p><strong>STEP 1:</strong> First, we simulate a $d$-dimensional feature vector,</p>
<script type="math/tex; mode=display">
X_{i} \stackrel{i i d}{\sim} \mathcal{N}(0, \Sigma)</script><p>where $\Sigma$ is a correlation matrix that is created using the vine method (4).</p>
<p><strong>STEP 2:</strong> Next, we create the potential outcomes according to</p>
<script type="math/tex; mode=display">
\begin{aligned}
&Y_{i}(1)=\mu_{1}\left(X_{i}\right)+\varepsilon_{i}(1) \\
&Y_{i}(0)=\mu_{0}\left(X_{i}\right)+\varepsilon_{i}(0)
\end{aligned}</script><p>where $\varepsilon_{i}(1), \varepsilon_{i}(0) \stackrel{i i d}{\sim} \mathcal{N}(0,1)$ and independent of $X_{i}$.</p>
<p><strong>STEP 3:</strong> Finally, we simulate the treatment assignment according to</p>
<script type="math/tex; mode=display">
W_{i} \sim \operatorname{Bern}\left(e\left(X_{i}\right)\right),</script><p>we set $Y_{i}=Y\left(W_{i}\right)$, and we obtain $\left(X_{i}, W_{i}, Y_{i}\right) .^{\dagger}$<br>We train each CATE estimator on a training set of $N$ units, and we evaluate its performance against a test set of $10^{5}$ units for which we know the true CATE. We repeat each experiment 30 times, and we report the averages.</p>
<h3 id="3-1-The-unbalanced-case-with-a-simple-CATE"><a href="#3-1-The-unbalanced-case-with-a-simple-CATE" class="headerlink" title="3.1 The unbalanced case with a simple CATE"></a>3.1 The unbalanced case with a simple CATE</h3><p>We choose the propensity score to be constant and very small, e(x) = 0.01, such that on average only one percent of the units receive treatment.</p>
<p><strong>Simulation SI 1 (unbalanced treatment assignment).</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
e(x) &=0.01, \quad d=20, \\\\
\mu_{0}(x) &=x^{T} \beta+5 \mathbb{I}\left(x_{1}>0.5\right), \quad \text { with } \beta \sim \operatorname{Unif}\left([-5,5]^{20}\right), \\\\
\mu_{1}(x) &=\mu_{0}(x)+8 \mathbb{I}\left(x_{2}>0.1\right) .
\end{aligned}</script><p><img src="https://s2.loli.net/2022/05/15/zKTCIgBGOMyVW19.png" alt="截屏2022-05-15 11.41.14" style="zoom:40%;" /></p>
<ul>
<li>The X-learner performs particularly well when the treatment group sizes are very unbalanced.</li>
</ul>
<h3 id="3-2-Balanced-cases-without-confounding"><a href="#3-2-Balanced-cases-without-confounding" class="headerlink" title="3.2 Balanced cases without confounding"></a>3.2 <strong>Balanced cases without confounding</strong></h3><p>Next, let us analyze two extreme cases: In one of them the CATE function is very complex and in the other one the CATE function is equal to zero.</p>
<ol>
<li><p>Let us first consider the case where the treatment effect is as complex as the response functions in the sense that it does not satisfy regularity conditions (such as sparsity or linearity) that the response functions do not satisfy.</p>
<p><strong>Simulation SI 2 (complex linear).</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
e(x) &=0.5, \quad d=20, \\\\
\mu_{1}(x) &=x^{T} \beta_{1}, \text { with } \beta_{1} \sim \operatorname{Unif}\left([1,30]^{20}\right) \\\\
\mu_{0}(x) &=x^{T} \beta_{0}, \text { with } \beta_{0} \sim \operatorname{Unif}\left([1,30]^{20}\right) .
\end{aligned}</script><p>The second setup (complex non-linear) is motivated by (3). Here the response function are non-linear functions.<br><strong>Simulation SI 3 (complex non-linear).</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
e(x) &=0.5, \quad d=20 \\\\
\mu_{1}(x) &=\frac{1}{2} \varsigma\left(x_{1}\right) \varsigma\left(x_{2}\right) \\\\
\mu_{0}(x) &=-\frac{1}{2} \varsigma\left(x_{1}\right) \varsigma\left(x_{2}\right)
\end{aligned}</script><p>with</p>
<script type="math/tex; mode=display">
\varsigma(x)=\frac{2}{1+e^{-12(x-1 / 2)}}</script><p><img src="https://s2.loli.net/2022/05/15/DYt573WAOwR2L1K.png" alt="截屏2022-05-15 11.41.56" style="zoom:40%;" /></p>
<ul>
<li><p>In this case, it is best to separate the CATE estimation problem into the two problems of estimating μ0 and μ1 since there is nothing one can learn from the other assignment group. The T-learner follows exactly this strategy and should perform very well.</p>
</li>
<li><p>The S-learner, on the other hand, pools the data and needs to learn that the response function for the treatment and the response function for the control group are very different.</p>
</li>
<li><p>Another interesting insight is that choosing BART or RF as the base learner can matter a great deal. </p>
</li>
</ul>
</li>
</ol>
<ol>
<li><p>Let us now consider the other extreme where we choose the response functions to be equal. This leads to a zero treatment effect, which is very favorable for the S-learner.</p>
<p><strong>Simulation SI 4 (global linear).</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
e(x) &=0.5, \quad d=5 \\\\
\mu_{0}(x) &=x^{T} \beta, \text { with } \beta \sim \operatorname{Unif}\left([1,30]^{5}\right) \\\\
\mu_{1}(x) &=\mu_{0}(x)
\end{aligned}</script><p><strong>Simulation SI 5 (piecewise linear).</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
e(x) &=0.5, \quad d=20, \\\\
\mu_{0}(x) &= \begin{cases}x^{T} \beta_{l} & \text { if } x_{20}<-0.4 \\\\
x^{T} \beta_{m} & \text { if }-0.4 \leq x_{20} \leq 0.4 \\\\
x^{T} \beta_{u} & \text { if } 0.4<x_{20},\end{cases} \\\\
\mu_{1}(x) &=\mu_{0}(x),
\end{aligned}</script><p>with</p>
<script type="math/tex; mode=display">
\beta_{l}(i)= \begin{cases}\beta(i) & \text { if } i \leq 5 \\\\ 0 & \text { otherwise }\end{cases}</script><script type="math/tex; mode=display">
\beta_{m}(i)= \begin{cases}\beta(i) & \text { if } 6 \leq i \leq 10 \\\\ 0 & \text { otherwise }\end{cases}</script><script type="math/tex; mode=display">
\beta_{u}(i)= \begin{cases}\beta(i) & \text { if } 11 \leq i \leq 15 \\\\ 0 & \text { otherwise }\end{cases}</script><p>and</p>
<script type="math/tex; mode=display">
\beta \sim \operatorname{Unif}\left([-15,15]^{d}\right)</script><p><img src="https://s2.loli.net/2022/05/15/mQTWyfqvFlkSUwL.png" alt="截屏2022-05-15 11.42.27" style="zoom:40%;" /></p>
<ul>
<li>For both simulations, the CATE is globally 0. As expected, the S-learner performs very well. Since the treatment indicator is given no special role, algorithms such as the lasso and RFs can completely ignore the treatment assignment by not choosing/splitting on it. This is beneficial if the CATE is in many places 0.</li>
</ul>
</li>
</ol>
<h3 id="3-3-Confounding"><a href="#3-3-Confounding" class="headerlink" title="3.3 Confounding"></a>3.3 Confounding</h3><p>In the preceding examples, the propensity score was globally equal to some constant. This is a special case, and in many observational studies, we cannot assume this to be true.</p>
<p><strong>Simulation SI 6 (beta confounded).</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
e(x) &=\frac{1}{4}\left(1+\beta\left(x_{1}, 2,4\right)\right), \\\\
\mu_{0}(x) &=2 x_{1}-1 \\\\
\mu_{1}(x) &=\mu_{0}(x)
\end{aligned}</script><p><img src="https://s2.loli.net/2022/05/15/q76MyuYfrbNWZUd.png" alt="截屏2022-05-15 11.43.32" style="zoom:40%;" /></p>
<ul>
<li>Figure SI 5 shows that none of the algorithms performs significantly worse under confounding.</li>
</ul>
<h3 id="3-4-Comparison-of-Convergence-Rates"><a href="#3-4-Comparison-of-Convergence-Rates" class="headerlink" title="3.4 Comparison of Convergence Rates"></a>3.4 Comparison of Convergence Rates</h3><p>In this section, we provide conditions under which the X-learner can be proven to outperform the T-learner in terms of pointwise estimation rate. (<em>*details see<a target="_blank" rel="noopener" href="https://www.pnas.org/doi/suppl/10.1073/pnas.1804597116">《Supporting Information: Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning》</a></em>)</p>
<p><strong>1. unbalanced groups</strong></p>
<p>In many real-world applications, we observe that the number of control units is much larger than the number of treated units, $m \gg n$. In that case, the bound on the EMSE of the T-learner will be dominated by the regression problem for the treated response function,</p>
<script type="math/tex; mode=display">
\sup _{\mathcal{P} \in F} \operatorname{EMSE}\left(\mathcal{P}, \hat{\tau}_{T}^{n m}\right) \leq C_{1} n^{-a_{\mu}}</script><p>The EMSE of the X-learner, however, will be dominated by the regression problem for the imputed treatment effects and it will achieve a faster rate of $n^{-a_{\tau}}$,</p>
<script type="math/tex; mode=display">
\sup _{\mathcal{P} \in F} \operatorname{EMSE}\left(\mathcal{P}, \hat{\tau}_{X}^{n m}\right) \leq C_{2} n^{-a_{\tau}}</script><p>This is a substantial improvement on when $a_{\tau}&gt;a_{\mu}$, and it demonstrates that, in contrast to the $\mathrm{T}$-learner, the $\mathrm{X}$-learner can exploit structural conditions on the treatment effect. We therefore expect the X-learner to perform particularly well when one of the treatment groups is larger than the other=. </p>
<p><strong>2. Example When the CATE Is Linear</strong></p>
<blockquote>
<p>(Lipschitz continuous regression functions). Let $F^{L}$ be the class of distributions on $(X, Y) \in[0,1]^{d} \times \mathbb{R}$ such that:</p>
<ol>
<li>The features, $X_{i}$, are $i . i . d$. uniformly distributed in $[0,1]^{d}$.</li>
<li>The observed outcomes are given by<script type="math/tex; mode=display">
Y_{i}=\mu\left(X_{i}\right)+\varepsilon_{i}</script>where the $\varepsilon_{i}$ is independent and normally distributed with mean 0 and variance $\sigma^{2}$.</li>
<li>$X_{i}$ and $\varepsilon_{i}$ are independent.</li>
<li>The regression function $\mu$ is Lipschitz continuous with parameter L.</li>
</ol>
</blockquote>
<p>The optimal rate of convergence for the regression problem of estimating $x \mapsto \mathbb{E}[Y \mid X=x]$ in Definition SI 1 is $N^{-2 /(2+d)}$. Furthermore, the KNN algorithm with the right choice of the number of neighbors and the Nadaraya-Watson estimator with the right kernels achieve this rate, and they are thus minimax optimal for this regression problem.</p>
<p><strong>3. Other case</strong></p>
<p>If there are no regularity conditions on the CATE function and the response functions are Lipschitz continuous, then both the X- and T-learner obtain the same minimax optimal rate $\mathcal{O}\left(n^{2 /(2+d)}+m^{2 /(2+d)}\right)$.</p>
<h2 id="4-Applications"><a href="#4-Applications" class="headerlink" title="4 Applications"></a>4 Applications</h2><h3 id="4-1-Social-Pressure-and-Voter-Turnout"><a href="#4-1-Social-Pressure-and-Voter-Turnout" class="headerlink" title="4.1 Social Pressure and Voter Turnout"></a>4.1 Social Pressure and Voter Turnout</h3><ul>
<li><p>In a large field experiment, Gerber et al. (1) show that <strong>substantially higher turnout was observed among registered voters who received a mailing promising to publicize their turnout to their neighbors</strong>.</p>
</li>
<li><p>Fig. 2 presents the estimated treatment effects, using X-RF where the potential voters are grouped by their voting history. Fig. 2, Upper shows the proportion of voters with a significant positive (blue) and a significant negative (red) CATE estimate. We can see that there is evidence of a negative backlash among a small number of people who voted only once in thepast five elections before the general election in 2004.</p>
</li>
<li>Fig. 2, Lower shows the distribution of CATE estimates for each of the subgroups. If the number of mailers is limited, one should target potential voters who voted three times during the past five elections, since this group has the highest ATE and it is a very big group of potential voters.</li>
</ul>
<p><img src="https://s2.loli.net/2022/05/15/EK6OQPedAw7ISzN.png" alt="截屏2022-05-15 11.44.19" style="zoom:50%;" /></p>
<ul>
<li><p>S-, T-, and X-RF all provide similar CATE estimates. This is not surprising since the data set is very large and most of the covariates are discrete.</p>
<p><img src="https://s2.loli.net/2022/05/15/Oe6lHBn7E1XuUqN.png" alt="截屏2022-05-15 11.45.12" style="zoom:40%;" /></p>
</li>
<li><p>We conducted a data-inspired simulation study to see how these estimators would behave in smaller samples. Fig. 3 presents the results of this simulation. They show that, in small samples, both X- and S-RF outperform T-RF, with X-RF performing the best, as one may conjecture, given the unequal sample sizes.</p>
<p><img src="https://s2.loli.net/2022/05/15/XiuGCnAqaEc53RF.png" alt="截屏2022-05-15 11.45.50" style="zoom:50%;" /></p>
</li>
</ul>
<p><strong>Supporting Information</strong> </p>
<p>(<em>*details see<a target="_blank" rel="noopener" href="https://www.pnas.org/doi/suppl/10.1073/pnas.1804597116">《Supporting Information: Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning》</a></em>)</p>
<p><img src="https://s2.loli.net/2022/05/15/KTHA6hiUFE8pXkC.png" alt="截屏2022-05-15 11.46.40" style="zoom:40%;" /></p>
<ul>
<li>none of the methods provide the correct coverage. The smooth CIs have a slightly higher coverage but the intervals are also slightly longer. However, the smooth CIs are computationally much more expensive and need a lot of bootstrap samples to be stable. Hence we prefer the normal approximated CIs.</li>
<li>We find that the coverage and the average confidence interval length for the overlap test set is very similar to that of the previous simulation study, CI-Simulation 1. This is not surprising, because the two setups are very similar and the overlap condition is satisfied in both.</li>
<li>We observe that for all methods the confidence intervals are tighter and the coverage is much lower than on the data where we have overlap because they try to extrapolate into regions of the covariate space without information on the treatment group. This is a problematic finding and suggests that confidence interval estimation in observational data is extremely difficult and that a violation of the overlap condition can lead to invalid inferences.</li>
</ul>
<h3 id="4-2-Reducing-Transphobia"><a href="#4-2-Reducing-Transphobia" class="headerlink" title="4.2 Reducing Transphobia"></a>4.2 Reducing Transphobia</h3><ul>
<li>Broockman et al. (2, 27) show that <strong>brief (10 min) but highquality door-to-door conversations can markedly reduce prejudice against gender-nonconforming individuals for at least 3 mo</strong>.</li>
<li>The authors find an ATE of 0.22 (SE: 0.072, t stat: 3.1) on their transgender tolerance scale. The authors report finding no evidence of heterogeneity in the treatment effect that can be explained by the observed covariates. Their analysis is based on linear models (OLS, lasso, and elastic net) without basis expansions.</li>
<li>Fig. 4A presents our results for estimating the CATE, using X–RF. We find that there is strong evidence that the positive effect that the authors find is only found among a subset of respondents that can be targeted based on observed covariates. The average of our CATE estimates is within half a SD of the ATE that the authors report.</li>
</ul>
<p><img src="https://s2.loli.net/2022/05/15/k6FvnopYKjx79u4.png" alt="截屏2022-05-15 11.47.20" style="zoom:50%;" /></p>
<ul>
<li>Fig. 4B presents the estimates from T–RF. These estimates are similar to those of X-RF, but with a larger spread.</li>
<li>S-RF shrinks the treatment estimates toward zero. The covariates are strongly predictive of the outcomes, and the splits in the S-RF are mostly on the features rather than the treatment indicator, because they are more predictive of the observed outcomes than the treatment assignment.</li>
</ul>
<p><img src="https://s2.loli.net/2022/05/15/d7DnTsUk4QF19hG.png" alt="截屏2022-05-15 11.47.55" style="zoom:40%;" /></p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><ul>
<li><p>This paper reviewed metaalgorithms for CATE estimation including the S- and T-learners. It then introduced a metaalgorithm, the X-learner.</p>
</li>
<li><p>Although none of the metaalgorithms is always the best, the X-learner performs well overall, especially in the real-data examples.</p>
</li>
<li><p>In practice, in finite samples, there will always be gains to be had if one accurately judges the underlying data-generating process. For example, if the treatment effect is simple, or even zero, then pooling the data across treatment and control conditions will be beneficial when estimating the response model (i.e., the S-learner will perform well). However, if the treatment effect is strongly heterogeneous and the response surfaces of the outcomes under treatment and control are very different, pooling the data will lead to worse finite sample performance (i.e., the T-learner will perform well).</p>
</li>
</ul>
<h2 id="Appendix-X-learner-with-R"><a href="#Appendix-X-learner-with-R" class="headerlink" title="Appendix: X-learner with R"></a>Appendix: X-learner with R</h2><p><code>*This is an example from the tutorial of Prof. Susan Athey, Stanford University</code></p>
<p><strong>Preparations</strong></p>
<p><strong>STEP 1: Fit the X-learner</strong></p>
<p>We will follow the algorithm outlined above very closely. Because the variable naming can be a bit cumbersome, let’s give some of our variables shorter aliases.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">X <span class="operator">&lt;-</span> df_train<span class="punctuation">[</span><span class="punctuation">,</span>covariate_names<span class="punctuation">]</span></span><br><span class="line">W <span class="operator">&lt;-</span> df_train<span class="operator">$</span>W</span><br><span class="line">Y <span class="operator">&lt;-</span> df_train<span class="operator">$</span>Y</span><br><span class="line">num.trees <span class="operator">&lt;-</span> 200  <span class="comment">#  We&#x27;ll make this a small number for speed here.</span></span><br><span class="line">n_train <span class="operator">&lt;-</span> <span class="built_in">dim</span><span class="punctuation">(</span>df_train<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># estimate separate response functions</span></span><br><span class="line">tf0 <span class="operator">&lt;-</span> regression_forest<span class="punctuation">(</span>X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> Y<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">,</span> num.trees<span class="operator">=</span>num.trees<span class="punctuation">)</span></span><br><span class="line">tf1 <span class="operator">&lt;-</span> regression_forest<span class="punctuation">(</span>X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> Y<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> num.trees<span class="operator">=</span>num.trees<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the &#x27;imputed treatment effects&#x27; using the other group</span></span><br><span class="line">D1 <span class="operator">&lt;-</span> Y<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">-</span> predict<span class="punctuation">(</span>tf0<span class="punctuation">,</span> X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">D0 <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>tf1<span class="punctuation">,</span> X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">$</span>predictions <span class="operator">-</span> Y<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the cross estimators </span></span><br><span class="line">xf0 <span class="operator">&lt;-</span> regression_forest<span class="punctuation">(</span>X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> D0<span class="punctuation">,</span> num.trees<span class="operator">=</span>num.trees<span class="punctuation">)</span></span><br><span class="line">xf1 <span class="operator">&lt;-</span> regression_forest<span class="punctuation">(</span>X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span> D1<span class="punctuation">,</span> num.trees<span class="operator">=</span>num.trees<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict treatment effects, making sure to always use OOB predictions where appropriate</span></span><br><span class="line">xf.preds.0 <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> n_train<span class="punctuation">)</span></span><br><span class="line">xf.preds.0<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">]</span> <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xf0<span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">xf.preds.0<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xf0<span class="punctuation">,</span> X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">xf.preds.1 <span class="operator">&lt;-</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> n_train<span class="punctuation">)</span></span><br><span class="line">xf.preds.1<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">0</span><span class="punctuation">]</span> <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xf0<span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">xf.preds.1<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xf0<span class="punctuation">,</span> X<span class="punctuation">[</span>W<span class="operator">==</span><span class="number">1</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimate the propensity score</span></span><br><span class="line">propf <span class="operator">&lt;-</span> regression_forest<span class="punctuation">(</span>X<span class="punctuation">,</span> W<span class="punctuation">,</span> num.trees<span class="operator">=</span>num.trees<span class="punctuation">)</span></span><br><span class="line">ehat <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>propf<span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line"></span><br><span class="line"><span class="comment"># Finally, compute the X-learner prediction</span></span><br><span class="line">tauhat_xl <span class="operator">&lt;-</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> ehat<span class="punctuation">)</span> <span class="operator">*</span> xf.preds.1 <span class="operator">+</span> ehat <span class="operator">*</span> xf.preds.0</span><br></pre></td></tr></table></figure>
<p><strong>STEP 2: Predict point estimates</strong></p>
<p>The function <code>EstimateCate</code> provides point estimates. To predict on a test set:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X.test <span class="operator">&lt;-</span> df_test<span class="punctuation">[</span><span class="punctuation">,</span>covariate_names<span class="punctuation">]</span></span><br><span class="line">ehat.test <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>propf<span class="punctuation">,</span> X.test<span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">xf.preds.1.test <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xf1<span class="punctuation">,</span> X.test<span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">xf.preds.0.test <span class="operator">&lt;-</span> predict<span class="punctuation">(</span>xf0<span class="punctuation">,</span> X.test<span class="punctuation">)</span><span class="operator">$</span>predictions</span><br><span class="line">tauhat_xl_test <span class="operator">&lt;-</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> ehat.test<span class="punctuation">)</span> <span class="operator">*</span> xf.preds.1.test <span class="operator">+</span> ehat.test <span class="operator">*</span> xf.preds.0.test</span><br></pre></td></tr></table></figure>
<p><strong>STEP 3: Compute confidence intervals</strong></p>
<p>Confidence intervals are computed via bootstrap. The process is straightforward but does not add any particular insight. We encourage the interested reader to see the algorithms in the paper for the exact implementation.</p>
<p><strong>(Preparations to be made)</strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>tidyverse<span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>tidyselect<span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>dplyr<span class="punctuation">)</span>       <span class="comment"># Data manipulation (0.8.0.1)</span></span><br><span class="line">library<span class="punctuation">(</span>fBasics<span class="punctuation">)</span>     <span class="comment"># Summary statistics (3042.89)</span></span><br><span class="line">library<span class="punctuation">(</span>corrplot<span class="punctuation">)</span>    <span class="comment"># Correlations (0.84)</span></span><br><span class="line">library<span class="punctuation">(</span>psych<span class="punctuation">)</span>       <span class="comment"># Correlation p-values (1.8.12)</span></span><br><span class="line">library<span class="punctuation">(</span>grf<span class="punctuation">)</span>         <span class="comment"># Generalized random forests (0.10.2)</span></span><br><span class="line">library<span class="punctuation">(</span>rpart<span class="punctuation">)</span>       <span class="comment"># Classification and regression trees, or CART (4.1-13)</span></span><br><span class="line">library<span class="punctuation">(</span>rpart.plot<span class="punctuation">)</span>  <span class="comment"># Plotting trees (3.0.6)</span></span><br><span class="line">library<span class="punctuation">(</span>treeClust<span class="punctuation">)</span>   <span class="comment"># Predicting leaf position for causal trees (1.1-7)</span></span><br><span class="line">library<span class="punctuation">(</span>car<span class="punctuation">)</span>         <span class="comment"># linear hypothesis testing for causal tree (3.0-2)</span></span><br><span class="line">library<span class="punctuation">(</span>devtools<span class="punctuation">)</span>    <span class="comment"># Install packages from github (2.0.1)</span></span><br><span class="line">library<span class="punctuation">(</span>readr<span class="punctuation">)</span>       <span class="comment"># Reading csv files (1.3.1)</span></span><br><span class="line">library<span class="punctuation">(</span>tidyr<span class="punctuation">)</span>       <span class="comment"># Database operations (0.8.3)</span></span><br><span class="line">library<span class="punctuation">(</span>tibble<span class="punctuation">)</span>      <span class="comment"># Modern alternative to data frames (2.1.1)</span></span><br><span class="line">library<span class="punctuation">(</span>knitr<span class="punctuation">)</span>       <span class="comment"># RMarkdown (1.21)</span></span><br><span class="line">library<span class="punctuation">(</span>kableExtra<span class="punctuation">)</span>  <span class="comment"># Prettier RMarkdown (1.0.1)</span></span><br><span class="line">library<span class="punctuation">(</span>ggplot2<span class="punctuation">)</span>     <span class="comment"># general plotting tool (3.1.0)</span></span><br><span class="line">library<span class="punctuation">(</span>haven<span class="punctuation">)</span>       <span class="comment"># read stata files (2.0.0)</span></span><br><span class="line">library<span class="punctuation">(</span>aod<span class="punctuation">)</span>         <span class="comment"># hypothesis testing (1.3.1)</span></span><br><span class="line">library<span class="punctuation">(</span>evtree<span class="punctuation">)</span>      <span class="comment"># evolutionary learning of globally optimal trees (1.0-7)</span></span><br><span class="line">library<span class="punctuation">(</span>purrr<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># R script for reading data from github repository, set path to where you have the tutorial files saved.</span></span><br><span class="line">source<span class="punctuation">(</span><span class="string">&#x27;load_data.R&#x27;</span><span class="punctuation">)</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Pick a dataset from the list above for parts I and II of the tutorial</span></span><br><span class="line">df_experiment <span class="operator">&lt;-</span> select_dataset<span class="punctuation">(</span><span class="string">&quot;welfare&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Combine all names</span></span><br><span class="line">all_variables_names <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span>outcome_variable_name<span class="punctuation">,</span> treatment_variable_name<span class="punctuation">,</span> covariate_names<span class="punctuation">)</span></span><br><span class="line">df <span class="operator">&lt;-</span> df_experiment <span class="operator">%&gt;%</span> select<span class="punctuation">(</span>all_variables_names<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Drop rows containing missing values</span></span><br><span class="line">df <span class="operator">&lt;-</span> df <span class="operator">%&gt;%</span> drop_na<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Rename variables</span></span><br><span class="line">df <span class="operator">&lt;-</span> df <span class="operator">%&gt;%</span> rename<span class="punctuation">(</span>Y<span class="operator">=</span>outcome_variable_name<span class="punctuation">,</span>W<span class="operator">=</span>treatment_variable_name<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Converting all columns to numerical and add row id</span></span><br><span class="line">df <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>lapply<span class="punctuation">(</span>df<span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span><span class="built_in">as.character</span><span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">df <span class="operator">&lt;-</span> df <span class="operator">%&gt;%</span> mutate_if<span class="punctuation">(</span><span class="built_in">is.character</span><span class="punctuation">,</span><span class="built_in">as.numeric</span><span class="punctuation">)</span></span><br><span class="line">df <span class="operator">&lt;-</span> df <span class="operator">%&gt;%</span> rowid_to_column<span class="punctuation">(</span> <span class="string">&quot;ID&quot;</span><span class="punctuation">)</span></span><br><span class="line">                        </span><br><span class="line">train_fraction <span class="operator">&lt;-</span> 0.80  <span class="comment"># Use train_fraction % of the dataset to train our models</span></span><br><span class="line"></span><br><span class="line">df_train <span class="operator">&lt;-</span> sample_frac<span class="punctuation">(</span>df<span class="punctuation">,</span> replace<span class="operator">=</span><span class="built_in">F</span><span class="punctuation">,</span> size<span class="operator">=</span>train_fraction<span class="punctuation">)</span></span><br><span class="line">df_test <span class="operator">&lt;-</span> anti_join<span class="punctuation">(</span>df<span class="punctuation">,</span>df_train<span class="punctuation">,</span> by <span class="operator">=</span> <span class="string">&quot;ID&quot;</span><span class="punctuation">)</span><span class="comment">#need to check on larger datasets                  </span></span><br></pre></td></tr></table></figure></div><div class="article-licensing box"><div class="licensing-title"><p>An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》</p><p><a href="https://ghmzhu.github.io/2022/05/20/metalearner/">https://ghmzhu.github.io/2022/05/20/metalearner/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Gehuiming Zhu</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-05-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-07-29</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Machine-Learning/">Machine Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Reading-Notes/">Reading Notes</a><a class="link-muted mr-2" rel="tag" href="/tags/Causal-Inference/">Causal Inference</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=62879b033b2aa70012029e9b&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/05/21/GMMCityClustering/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">基于高斯混合模型的中国城市聚类研究</span></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Reading-Notes/"><span class="level-start"><span class="level-item">Reading Notes</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Reading-Notes/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Research/"><span class="level-start"><span class="level-item">Research</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Research/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Research/Mathematics/"><span class="level-start"><span class="level-item">Mathematics</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Research/Urban-Governance/"><span class="level-start"><span class="level-item">Urban Governance</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-22T10:29:11.000Z">2022-10-22</time></p><p class="title"><a href="/2022/10/22/mlbo-notes/">《机器学习：贝叶斯和优化方法》阅读笔记</a></p><p class="categories"><a href="/categories/Reading-Notes/">Reading Notes</a> / <a href="/categories/Reading-Notes/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-22T05:20:58.000Z">2022-10-22</time></p><p class="title"><a href="/2022/10/22/brml-notes/">《Bayesian Reasoning and Machine Learning》阅读笔记</a></p><p class="categories"><a href="/categories/Reading-Notes/">Reading Notes</a> / <a href="/categories/Reading-Notes/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-28T15:05:44.000Z">2022-07-28</time></p><p class="title"><a href="/2022/07/28/health-hte/">生活圈状况对健康水平的因果效应异质性探究——基于机器学习方法的检验</a></p><p class="categories"><a href="/categories/Research/">Research</a> / <a href="/categories/Research/Machine-Learning/">Machine Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-04T13:39:28.000Z">2022-07-04</time></p><p class="title"><a href="/2022/07/04/TDA-Finance/">Research Progress Report for Topological Data Analysis</a></p><p class="categories"><a href="/categories/Research/">Research</a> / <a href="/categories/Research/Mathematics/">Mathematics</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-21T10:52:21.000Z">2022-05-21</time></p><p class="title"><a href="/2022/05/21/GMMCityClustering/">基于高斯混合模型的中国城市聚类研究</a></p><p class="categories"><a href="/categories/Research/">Research</a> / <a href="/categories/Research/Urban-Governance/">Urban Governance</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Causal-Inference/"><span class="tag">Causal Inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mathematics/"><span class="tag">Mathematics</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Quant/"><span class="tag">Quant</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reading-Notes/"><span class="tag">Reading Notes</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Research/"><span class="tag">Research</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Urban-Governance/"><span class="tag">Urban Governance</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/profilephoto.JPG" alt="Gehuiming Zhu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Gehuiming Zhu</p><p class="is-size-6 is-block">Student, Fudan University</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ghmzhu" target="_blank" rel="noopener">Follow</a></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.jpg" alt="Gehuiming Zhu&#039;s Site" height="28"></a><p class="is-size-7"><span>&copy; 2022 Gehuiming Zhu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ghmzhu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>