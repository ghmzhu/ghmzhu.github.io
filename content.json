{"pages":[{"title":"About","text":"Education2021-2024 Master of Economics, SSDPP, Fudan University 2016-2020 Bachelor of Engineering, Dept. of Computer Science, Wuhan University 2016-2020 Bachelor of Economics, Dept. of Finance, Wuhan University Internship2021 Technical System Group, Technical Committee, Xiaomi Co.,Ltd. 2020 Dept. of Terminal Management, China UnionPay Merchant Services Co., Ltd. Research Fields 量化金融 | Quantitative Finance 宏观经济 | Macro Economins 城市治理 | Urban Governance 机器学习 | Machine Learning 大数据分析 | Big Data Analytics","link":"/about/index.html"},{"title":"My Photography","text":"I’ve been addicted to photography since my undergraduate period. This page records some of the images I shot all over China, and whenever I see them, I can think of the beautiful moments when I press the shutter.Please feel free to download the compressed pictures for your enjoyment, but please note that these pictures are protected by copyright and cannot be used for commerce or misappropriation. Wuhan Beijing Shanghai","link":"/Photography/index.html"}],"posts":[{"title":"An interpretation of 《Metalearners for estimating heterogeneous treatment effects using machine learning》","text":"“There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms—such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks—to estimate the CATE, a function that the base algorithms are not designed to estimate directly.” 1 Brief Introduction1.1 Why Machine Learning? “同样是拟合一个线性回归模型，传统的社会科学研究者将注意力放在这个模型中特定自变量的系数上，而机器学习的目的则是看这个回归模型多大程度上可以预测因变量的取值。这种关注点上的区分非常重要。因为我们在进行模型拟合时所需要特别关注的问题（例如共线性等）在机器学习的分析范式下便不再是问题。只要有助于提升预测的准确度，我们的模型拟合过程完全可以变得非常有弹性。” “所谓的‘因果推断的基本问题’，本质上是一个缺失值问题，而为了解决缺失值问题，我们就需要利用已有的资料进行某种意义上的“预测”，这恰恰是机器学习方法的强项所在。” ——胡安宁，《中国社会科学报》 传统回归模型的交互项分析主要有两个问题：交互项不能无限制添加，以及交互项的设定具有主观性。倾向值方法存在模型和系数不确定，以及无法确定具体起异质化作用的混淆变量的问题。 ——胡安宁,吴晓刚,陈云松, 《处理效应异质性分析——机器学习方法带来的机遇与挑战》 Casual Tree: Athey S, Imbens G. Recursive partitioning for heterogeneous causal effects[J]. Proceedings of the National Academy of Sciences, 2016, 113(27): 7353-7360. Casual Forests: Wager S, Athey S. Estimation and inference of heterogeneous treatment effects using random forests[J]. Journal of the American Statistical Association, 2018, 113(523): 1228-1242. BART: Chipman H A , Mcculloch G R E . BART: BAYESIAN ADDITIVE REGRESSION TREES[J]. Annals of Applied Statistics, 2010, 4(1):266-298. Transfer Learning: Künzel S R, Stadie B C, Vemuri N, et al. Transfer learning for estimating causal effects using neural networks[J]. arXiv preprint arXiv:1808.07804, 2018. 1.2 Framework and Definitions We use the following representation of $\\mathcal{P}$ :$$\\begin{aligned}X &amp; \\sim \\Lambda \\\\W &amp; \\sim \\operatorname{Bern}(e(X)) \\\\Y(0) &amp;=\\mu_{0}(X)+\\varepsilon(0) \\\\Y(1) &amp;=\\mu_{1}(X)+\\varepsilon(1)\\end{aligned}$$ $\\left(Y_{i}(0), Y_{i}(1), X_{i}, W_{i}\\right) \\sim \\mathcal{P}$, where $X_{i} \\in \\mathbb{R}^{d}$ is a $d$-dimensional covariate or feature vector, $W_{i} \\in{0,1}$ is the treatment-assignment indicator (to be defined precisely later), $Y_{i}(0) \\in \\mathbb{R}$ is the potential outcome of unit $i$ when $i$ is assigned to the control group, and $Y_{i}(1)$ is the potential outcome when $i$ is assigned to the treatment group. With this definition, the ATE is defined as $$\\mathrm{ATE}:=\\mathbb{E}[Y(1)-Y(0)]$$ the response under control $\\mu_{0}(x):=\\mathbb{E}[Y(0) \\mid X=x] \\quad$ the response under treatment$\\quad \\mu_{1}(x):=\\mathbb{E}[Y(1) \\mid X=x]$ For a new unit $i$ with covariate vector $x_{i}$, to decide whether to give the unit the treatment, we wish to estimate the ITE of unit $i, D_{i}$, which is defined as $$D_{i}:=Y_{i}(1)-Y_{i}(0) .$$ the CATE function is defined as $$\\tau(x):=\\mathbb{E}[D \\mid X=x]=\\mathbb{E}[Y(1)-Y(0) \\mid X=x],$$ Condition 1:$$(\\varepsilon(0), \\varepsilon(1)) \\perp W \\mid X .$$Condition 2: There exists $e_{\\min }$ and $e_{\\max }$, such that for all $x$ in the support of $X$,$$0&lt;e_{\\min }&lt;e(x)&lt;e_{\\max }&lt;1 .$$ If we denote our learned estimates as $\\hat{\\mu}_{0}(x)$ and $\\hat{\\mu}_{1}(x)$, then we can form the CATE estimate as the difference between the two$$\\hat{\\tau}(x)=\\hat{\\mu}_{1}(x)-\\hat{\\mu}_{0}(x) .$$ In this work, we are interested in estimators with a small expected mean squared error (EMSE) for estimating the CATE, $$\\operatorname{EMSE}(\\mathcal{P}, \\hat{\\tau})=\\mathbb{E}\\left[(\\tau(\\mathcal{X})-\\hat{\\tau}(\\mathcal{X}))^{2}\\right] .$$ HINT: We can never observe the heterogeneous treatment effects because we can’t observe the counterfactual, we can only estimate the heterogeneous treatment effects, so we can use the modified MSE formula:$$\\operatorname{EMSE}(\\mathcal{P}, \\hat{\\tau})=\\mathbb{E}\\left[(\\tau(\\mathcal{X})-\\hat{\\tau}(\\mathcal{X}))^{2}-(\\tau(\\mathcal{X}))^{2}\\right] .$$(see Susan Atheya and Guido Imbensa,《Recursive partitioning for heterogeneous causal effects》) 2 Metaalgorithms2.1 Ensemble Learning In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Bagging (Bootstrap aggregating) Bagging使用装袋采样来获取数据子集训练基础学习器。通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。例如Random Forest、本文的metalearner等。 Boosting Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是利用加权的数据。在训练的早期对于错分数据给予较大的权重。比较经典的有AdaBoost。 2.2 T-learner Step1: 用一个基学习器来拟合对照组的相应函数，$\\mu_0(x)=\\mathbb{E}[Y(0)|X=x]$。基学习器可以在对照组的样本${(X_i,Y_i)}_{W_i=0}$采用任何监督学习或者回归估计，我们用符号$\\hat{\\mu_0}$表示。 Step2: 我们估计treatment的相应函数，$\\mu_1(x)=\\mathbb{E}[Y(1)|X=x]$,在实验组的数据上进行训练，我们用符号$\\hat{\\mu_1}$来表示。T-learner可以通过以下公式得出：$$\\hat{\\tau_T}(x)=\\hat{\\mu_1}(x) -\\hat{\\mu_0}(x) \\ \\tag{3}$$ 2.3 S-learner treatment是被当做特征使用的。评估公式为$\\mu(x,w) := \\mathbb{E}[Y^{abs}|X=x,W=w]$,这里可以用任何基学习器，我们用$\\hat{\\mu}$表示模型的估计量，因此CATE估计量表示为：$$\\hat{\\tau_S}(x) =\\hat{\\mu}(x,1) -\\hat{\\mu}(x,0) \\tag{4}$$ 2.4 X-learner STEP 1: 用任意的监督学习或者回归算法估计相应函数，用$\\hat{\\mu_0}$和$\\hat{\\mu_1}$表示估计量。$$\\mu_0 = \\mathbb{E}[Y(0)|X=x] \\tag{5}$$ $$\\mu_1 = \\mathbb{E}[Y(1)|X=x] \\tag{6}$$ STEP 2: 根据对照组的模型来计算实验组中的个人treatment效果，根据实验组的模型来计算实验最中的个人treatment效果。用公式表示为：$$\\tilde{D_i^1} :=Y_i^1-\\hat{\\mu_0}(X_i^1) \\tag{7}$$ $$\\tilde{D_i^0} :=\\hat{\\mu_1}(X_i^0)-Y_i^0 \\tag{8}$$ 注意到，如果$\\hat{\\mu_0}=\\mu_0$和$\\hat{\\mu_1}=\\mu_1$，则$\\tau(x)=\\mathbb{E}[\\tilde{D^1}|X=x]=\\mathbb{E}[\\tilde{D^0}|X=x]$ 使用任意的监督学习或者回归算法计算$\\tau(x)$有两种方式：一种是利用treatment组训练的模型计算得到的$\\hat{\\tau_1}(x)$,另一种是利用对照组训练的模型计算得到的$\\hat{\\tau_0}(x)$. STEP 3: 通过阶段2中计算得到的两个估计量进行加权计算CATE估计量： $g \\in [0,1]$是一个权重函数。 2.5 Intuition Behind the Metalearners Fig. 1A shows the outcome for units in the treatment group (circles) and the outcome of units in the untreated group (crosses). In this example, the CATE is constant and equal to one. The T-learner would now use estimator $\\hat{\\tau}_{T}(x)=\\hat{\\mu}_{1}(x)-\\hat{\\mu}_{0}(x)$ (Fig. 1C, solid line), ==which is a relatively complicated function== with jumps at 0 and 0.5, while the true ${\\tau}(x)$ is a constant. For X-learner, If we choose $g(x)=\\hat{e}(x)$, an estimator for the propensity score, ==$\\hat{\\tau}$ will be very similar to $\\hat{\\tau}_{1}(x)$==, since we have many more observations in the control group; i.e., $\\hat{e}(x)$ is small. 在这个例子中我们选择S-learner很难评估，例如当用RF的基学习器进行训练时，S-learner第一个split可能把97.5%的实验组的样本split出去，造成后续split时缺少实验组的样本。换句话说就是实验组和对照组的样本比例极不均衡时，如果使用S-learner训练时几次split就会把所有的实验组样本使用完。 3 Simulation ResultsSTEP 1: First, we simulate a $d$-dimensional feature vector,$$X_{i} \\stackrel{i i d}{\\sim} \\mathcal{N}(0, \\Sigma)$$where $\\Sigma$ is a correlation matrix that is created using the vine method (4). STEP 2: Next, we create the potential outcomes according to$$\\begin{aligned}&amp;Y_{i}(1)=\\mu_{1}\\left(X_{i}\\right)+\\varepsilon_{i}(1) \\&amp;Y_{i}(0)=\\mu_{0}\\left(X_{i}\\right)+\\varepsilon_{i}(0)\\end{aligned}$$where $\\varepsilon_{i}(1), \\varepsilon_{i}(0) \\stackrel{i i d}{\\sim} \\mathcal{N}(0,1)$ and independent of $X_{i}$. STEP 3: Finally, we simulate the treatment assignment according to$$W_{i} \\sim \\operatorname{Bern}\\left(e\\left(X_{i}\\right)\\right),$$we set $Y_{i}=Y\\left(W_{i}\\right)$, and we obtain $\\left(X_{i}, W_{i}, Y_{i}\\right) .^{\\dagger}$We train each CATE estimator on a training set of $N$ units, and we evaluate its performance against a test set of $10^{5}$ units for which we know the true CATE. We repeat each experiment 30 times, and we report the averages. 3.1 The unbalanced case with a simple CATEWe choose the propensity score to be constant and very small, e(x) = 0.01, such that on average only one percent of the units receive treatment. Simulation SI 1 (unbalanced treatment assignment).$$\\begin{aligned}e(x) &amp;=0.01, \\quad d=20, \\\\\\mu_{0}(x) &amp;=x^{T} \\beta+5 \\mathbb{I}\\left(x_{1}&gt;0.5\\right), \\quad \\text { with } \\beta \\sim \\operatorname{Unif}\\left([-5,5]^{20}\\right), \\\\\\mu_{1}(x) &amp;=\\mu_{0}(x)+8 \\mathbb{I}\\left(x_{2}&gt;0.1\\right) .\\end{aligned}$$ The X-learner performs particularly well when the treatment group sizes are very unbalanced. 3.2 Balanced cases without confoundingNext, let us analyze two extreme cases: In one of them the CATE function is very complex and in the other one the CATE function is equal to zero. Let us first consider the case where the treatment effect is as complex as the response functions in the sense that it does not satisfy regularity conditions (such as sparsity or linearity) that the response functions do not satisfy. Simulation SI 2 (complex linear).$$\\begin{aligned}e(x) &amp;=0.5, \\quad d=20, \\\\\\mu_{1}(x) &amp;=x^{T} \\beta_{1}, \\text { with } \\beta_{1} \\sim \\operatorname{Unif}\\left([1,30]^{20}\\right) \\\\\\mu_{0}(x) &amp;=x^{T} \\beta_{0}, \\text { with } \\beta_{0} \\sim \\operatorname{Unif}\\left([1,30]^{20}\\right) .\\end{aligned}$$The second setup (complex non-linear) is motivated by (3). Here the response function are non-linear functions.Simulation SI 3 (complex non-linear).$$\\begin{aligned}e(x) &amp;=0.5, \\quad d=20 \\\\\\mu_{1}(x) &amp;=\\frac{1}{2} \\varsigma\\left(x_{1}\\right) \\varsigma\\left(x_{2}\\right) \\\\\\mu_{0}(x) &amp;=-\\frac{1}{2} \\varsigma\\left(x_{1}\\right) \\varsigma\\left(x_{2}\\right)\\end{aligned}$$with$$\\varsigma(x)=\\frac{2}{1+e^{-12(x-1 / 2)}}$$ In this case, it is best to separate the CATE estimation problem into the two problems of estimating μ0 and μ1 since there is nothing one can learn from the other assignment group. ==The T-learner follows exactly this strategy and should perform very well==. ==The S-learner, on the other hand, pools the data== and needs to learn that the response function for the treatment and the response function for the control group are very different. Another interesting insight is that ==choosing BART or RF as the base learner can matter a great deal==. Let us now consider the other extreme where we choose the response functions to be equal. This leads to a zero treatment effect, which is very favorable for the S-learner. Simulation SI 4 (global linear).$$\\begin{aligned}e(x) &amp;=0.5, \\quad d=5 \\\\\\mu_{0}(x) &amp;=x^{T} \\beta, \\text { with } \\beta \\sim \\operatorname{Unif}\\left([1,30]^{5}\\right) \\\\\\mu_{1}(x) &amp;=\\mu_{0}(x)\\end{aligned}$$Simulation SI 5 (piecewise linear).$$\\begin{aligned}e(x) &amp;=0.5, \\quad d=20, \\\\\\mu_{0}(x) &amp;= \\begin{cases}x^{T} \\beta_{l} &amp; \\text { if } x_{20}&lt;-0.4 \\\\x^{T} \\beta_{m} &amp; \\text { if }-0.4 \\leq x_{20} \\leq 0.4 \\\\x^{T} \\beta_{u} &amp; \\text { if } 0.4&lt;x_{20},\\end{cases} \\\\\\mu_{1}(x) &amp;=\\mu_{0}(x),\\end{aligned}$$with$$\\beta_{l}(i)= \\begin{cases}\\beta(i) &amp; \\text { if } i \\leq 5 \\\\ 0 &amp; \\text { otherwise }\\end{cases}$$$$\\beta_{m}(i)= \\begin{cases}\\beta(i) &amp; \\text { if } 6 \\leq i \\leq 10 \\\\ 0 &amp; \\text { otherwise }\\end{cases}$$ $$\\beta_{u}(i)= \\begin{cases}\\beta(i) &amp; \\text { if } 11 \\leq i \\leq 15 \\\\ 0 &amp; \\text { otherwise }\\end{cases}$$ and$$\\beta \\sim \\operatorname{Unif}\\left([-15,15]^{d}\\right)$$ For both simulations, the CATE is globally 0. As expected, ==the S-learner performs very well==. Since the treatment indicator is given no special role, algorithms such as the lasso and RFs can completely ignore the treatment assignment by not choosing/splitting on it. This is beneficial if the CATE is in many places 0. 3.3 ConfoundingIn the preceding examples, the propensity score was globally equal to some constant. This is a special case, and in many observational studies, we cannot assume this to be true. Simulation SI 6 (beta confounded).$$\\begin{aligned}e(x) &amp;=\\frac{1}{4}\\left(1+\\beta\\left(x_{1}, 2,4\\right)\\right), \\\\\\mu_{0}(x) &amp;=2 x_{1}-1 \\\\\\mu_{1}(x) &amp;=\\mu_{0}(x)\\end{aligned}$$ Figure SI 5 shows that none of the algorithms performs significantly worse under confounding. 3.4 Comparison of Convergence RatesIn this section, we provide conditions under which the X-learner can be proven to outperform the T-learner in terms of pointwise estimation rate. (*details see《Supporting Information: Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning》) 1. unbalanced groups In many real-world applications, we observe that the number of control units is much larger than the number of treated units, $m \\gg n$. In that case, the bound on the EMSE of the T-learner will be dominated by the regression problem for the treated response function,$$\\sup _{\\mathcal{P} \\in F} \\operatorname{EMSE}\\left(\\mathcal{P}, \\hat{\\tau}_{T}^{n m}\\right) \\leq C_{1} n^{-a_{\\mu}}$$The EMSE of the X-learner, however, will be dominated by the regression problem for the imputed treatment effects and it will achieve a faster rate of $n^{-a_{\\tau}}$,$$\\sup _{\\mathcal{P} \\in F} \\operatorname{EMSE}\\left(\\mathcal{P}, \\hat{\\tau}_{X}^{n m}\\right) \\leq C_{2} n^{-a_{\\tau}}$$This is a substantial improvement on when $a_{\\tau}&gt;a_{\\mu}$, and it demonstrates that, in contrast to the $\\mathrm{T}$-learner, the $\\mathrm{X}$-learner can exploit structural conditions on the treatment effect. ==We therefore expect the X-learner to perform particularly well when one of the treatment groups is larger than the other==. 2. Example When the CATE Is Linear (Lipschitz continuous regression functions). Let $F^{L}$ be the class of distributions on $(X, Y) \\in[0,1]^{d} \\times \\mathbb{R}$ such that: The features, $X_{i}$, are $i . i . d$. uniformly distributed in $[0,1]^{d}$. The observed outcomes are given by$$Y_{i}=\\mu\\left(X_{i}\\right)+\\varepsilon_{i}$$where the $\\varepsilon_{i}$ is independent and normally distributed with mean 0 and variance $\\sigma^{2}$. $X_{i}$ and $\\varepsilon_{i}$ are independent. The regression function $\\mu$ is Lipschitz continuous with parameter L. The optimal rate of convergence for the regression problem of estimating $x \\mapsto \\mathbb{E}[Y \\mid X=x]$ in Definition SI 1 is $N^{-2 /(2+d)}$. Furthermore, the KNN algorithm with the right choice of the number of neighbors and the Nadaraya-Watson estimator with the right kernels achieve this rate, and they are thus minimax optimal for this regression problem. 3. Other case If there are no regularity conditions on the CATE function and the response functions are Lipschitz continuous, then both the X- and T-learner obtain the same minimax optimal rate $\\mathcal{O}\\left(n^{2 /(2+d)}+m^{2 /(2+d)}\\right)$. 4 Applications4.1 Social Pressure and Voter Turnout In a large field experiment, Gerber et al. (1) show that substantially higher turnout was observed among registered voters who received a mailing promising to publicize their turnout to their neighbors. Fig. 2 presents the estimated treatment effects, using X-RF where the potential voters are grouped by their voting history. Fig. 2, Upper shows the proportion of voters with a significant positive (blue) and a significant negative (red) CATE estimate. ==We can see that there is evidence of a negative backlash among a small number of people who voted only once in thepast five elections before the general election in 2004==. Fig. 2, Lower shows the distribution of CATE estimates for each of the subgroups. ==If the number of mailers is limited, one should target potential voters who voted three times during the past five elections==, since this group has the highest ATE and it is a very big group of potential voters. ==S-, T-, and X-RF all provide similar CATE estimates.== This is not surprising since the data set is very large and most of the covariates are discrete. We conducted a data-inspired simulation study to see how these estimators would behave in smaller samples. Fig. 3 presents the results of this simulation. They show that, ==in small samples, both X- and S-RF outperform T-RF, with X-RF performing the best==, as one may conjecture, given the unequal sample sizes. Supporting Information (*details see《Supporting Information: Meta-learners for Estimating Heterogeneous Treatment Effects using Machine Learning》) none of the methods provide the correct coverage. The smooth CIs have a slightly higher coverage but the intervals are also slightly longer. However, the smooth CIs are computationally much more expensive and need a lot of bootstrap samples to be stable. ==Hence we prefer the normal approximated CIs==. We find that ==the coverage and the average confidence interval length for the overlap test set is very similar to that of the previous simulation study==, CI-Simulation 1. This is not surprising, because the two setups are very similar and the overlap condition is satisfied in both. We observe that for all methods the confidence intervals are tighter and the coverage is much lower than on the data where we have overlap because they try to extrapolate into regions of the covariate space without information on the treatment group. This is a problematic finding and suggests that ==confidence interval estimation in observational data is extremely difficult and that a violation of the overlap condition can lead to invalid inferences==. 4.2 Reducing Transphobia Broockman et al. (2, 27) show that brief (10 min) but highquality door-to-door conversations can markedly reduce prejudice against gender-nonconforming individuals for at least 3 mo. The authors find an ATE of 0.22 (SE: 0.072, t stat: 3.1) on their transgender tolerance scale. The authors report finding ==no evidence of heterogeneity in the treatment effect== that can be explained by the observed covariates. Their analysis is based on linear models (OLS, lasso, and elastic net) without basis expansions. Fig. 4A presents our results for estimating the CATE, using X–RF. We find that there is strong evidence that ==the positive effect that the authors find is only found among a subset of respondents that can be targeted based on observed covariates.== The average of our CATE estimates is within half a SD of the ATE that the authors report. Fig. 4B presents the estimates from T–RF. These estimates are similar to those of X-RF, but with a larger spread. ==S-RF shrinks the treatment estimates toward zero.== The covariates are strongly predictive of the outcomes, and the splits in the S-RF are mostly on the features rather than the treatment indicator, because they are more predictive of the observed outcomes than the treatment assignment. 5 Conclusion This paper reviewed metaalgorithms for CATE estimation including the S- and T-learners. It then introduced a metaalgorithm, the X-learner. Although none of the metaalgorithms is always the best, ==the X-learner performs well overall==, especially in the real-data examples. ==In practice, in finite samples, there will always be gains to be had if one accurately judges the underlying data-generating process.== For example, if the treatment effect is simple, or even zero, then pooling the data across treatment and control conditions will be beneficial when estimating the response model (i.e., the S-learner will perform well). However, if the treatment effect is strongly heterogeneous and the response surfaces of the outcomes under treatment and control are very different, pooling the data will lead to worse finite sample performance (i.e., the T-learner will perform well). Appendix: X-learner with R*This is an example from the tutorial of Prof. Susan Athey, Stanford University Preparations STEP 1: Fit the X-learner We will follow the algorithm outlined above very closely. Because the variable naming can be a bit cumbersome, let’s give some of our variables shorter aliases. 1234567891011121314151617181920212223242526272829303132X &lt;- df_train[,covariate_names]W &lt;- df_train$WY &lt;- df_train$Ynum.trees &lt;- 200 # We'll make this a small number for speed here.n_train &lt;- dim(df_train)[1]# estimate separate response functionstf0 &lt;- regression_forest(X[W==0,], Y[W==0], num.trees=num.trees)tf1 &lt;- regression_forest(X[W==1,], Y[W==1], num.trees=num.trees)# Compute the 'imputed treatment effects' using the other groupD1 &lt;- Y[W==1] - predict(tf0, X[W==1,])$predictionsD0 &lt;- predict(tf1, X[W==0,])$predictions - Y[W==0]# Compute the cross estimators xf0 &lt;- regression_forest(X[W==0,], D0, num.trees=num.trees)xf1 &lt;- regression_forest(X[W==1,], D1, num.trees=num.trees)# Predict treatment effects, making sure to always use OOB predictions where appropriatexf.preds.0 &lt;- rep(0, n_train)xf.preds.0[W==0] &lt;- predict(xf0)$predictionsxf.preds.0[W==1] &lt;- predict(xf0, X[W==1,])$predictionsxf.preds.1 &lt;- rep(0, n_train)xf.preds.1[W==0] &lt;- predict(xf0)$predictionsxf.preds.1[W==1] &lt;- predict(xf0, X[W==1,])$predictions# Estimate the propensity scorepropf &lt;- regression_forest(X, W, num.trees=num.trees)ehat &lt;- predict(propf)$predictions# Finally, compute the X-learner predictiontauhat_xl &lt;- (1 - ehat) * xf.preds.1 + ehat * xf.preds.0 STEP 2: Predict point estimates The function EstimateCate provides point estimates. To predict on a test set: 12345X.test &lt;- df_test[,covariate_names]ehat.test &lt;- predict(propf, X.test)$predictionsxf.preds.1.test &lt;- predict(xf1, X.test)$predictionsxf.preds.0.test &lt;- predict(xf0, X.test)$predictionstauhat_xl_test &lt;- (1 - ehat.test) * xf.preds.1.test + ehat.test * xf.preds.0.test STEP 3: Compute confidence intervals Confidence intervals are computed via bootstrap. The process is straightforward but does not add any particular insight. We encourage the interested reader to see the algorithms in the paper for the exact implementation. (Preparations to be made) 12345678910111213141516171819202122library(tidyverse)library(tidyselect)library(dplyr) # Data manipulation (0.8.0.1)library(fBasics) # Summary statistics (3042.89)library(corrplot) # Correlations (0.84)library(psych) # Correlation p-values (1.8.12)library(grf) # Generalized random forests (0.10.2)library(rpart) # Classification and regression trees, or CART (4.1-13)library(rpart.plot) # Plotting trees (3.0.6)library(treeClust) # Predicting leaf position for causal trees (1.1-7)library(car) # linear hypothesis testing for causal tree (3.0-2)library(devtools) # Install packages from github (2.0.1)library(readr) # Reading csv files (1.3.1)library(tidyr) # Database operations (0.8.3)library(tibble) # Modern alternative to data frames (2.1.1)library(knitr) # RMarkdown (1.21)library(kableExtra) # Prettier RMarkdown (1.0.1)library(ggplot2) # general plotting tool (3.1.0)library(haven) # read stata files (2.0.0)library(aod) # hypothesis testing (1.3.1)library(evtree) # evolutionary learning of globally optimal trees (1.0-7)library(purrr) 12345678910111213141516171819202122232425# R script for reading data from github repository, set path to where you have the tutorial files saved.source('load_data.R') # Pick a dataset from the list above for parts I and II of the tutorialdf_experiment &lt;- select_dataset(&quot;welfare&quot;)# Combine all namesall_variables_names &lt;- c(outcome_variable_name, treatment_variable_name, covariate_names)df &lt;- df_experiment %&gt;% select(all_variables_names)# Drop rows containing missing valuesdf &lt;- df %&gt;% drop_na()# Rename variablesdf &lt;- df %&gt;% rename(Y=outcome_variable_name,W=treatment_variable_name)# Converting all columns to numerical and add row iddf &lt;- data.frame(lapply(df, function(x) as.numeric(as.character(x))))df &lt;- df %&gt;% mutate_if(is.character,as.numeric)df &lt;- df %&gt;% rowid_to_column( &quot;ID&quot;) train_fraction &lt;- 0.80 # Use train_fraction % of the dataset to train our modelsdf_train &lt;- sample_frac(df, replace=F, size=train_fraction)df_test &lt;- anti_join(df,df_train, by = &quot;ID&quot;)#need to check on larger datasets","link":"/2022/05/20/metalearner/"},{"title":"Research Progress Report for Topological Data Analysis","text":"Topological Data Analysis (TDA) is one of the emerging research methods in recent years, but its application in the financial field is not common. Since 2019, we began to use TDA to study The Chinese stock market and innovatively proposed the method of Modified TDA. This report summarizes some of our progress so far. Note: This report is only available to designated people, such as advisors, interviewers, etc. You can use the key we provide to unlock the PDF. Do not circulate the report or keep it for a long time. The report is only for recording our current research progress, rather than a draft paper. We plan to make major changes to some parts of it before we finish the draft, for example, some basic mathematical concepts that we directly refer to or simply rewrite. This PDF reader only supports desktop Chrome, Firefox, and other browsers. It does not support Safari or various mobile terminals.","link":"/2022/07/04/TDA-Finance/"},{"title":"基于高斯混合模型的中国城市聚类研究","text":"这篇文章为我在《SOCI630101: 人口迁移与城市化研究2021》课程中写作的期末报告，也算是用高斯混合模型用于城市研究的一个小demo。代码已开源至[Github]。 Abstract: 城市等级划分对于厘清城市之间的差异，制定科学规划有重要意义，但实际上目前城市等级划分层出不穷，甚至自相矛盾，缺乏可以用于学术研究的通过严谨计算得到的城市等级标签。为此，国家统计局城市司提出，要制定科学统一的“X线城市”分级标准。本文旨在提出一种符合学术规范的城市分级方法，采用高斯混合模型，依据中国290个地级市的人口、GDP、财政、灯光等数据，将中国城市划分为若干等级，且每个等级内的城市符合一个多维高斯分布。 1 介绍不同的城市往往具有不同等级的房价[Rappaport et al., 2007]、GDP、人口、财政等数据, 因此我们认为中国城市可以被分为若干个等级，每个等级内的城市具有各维度高度的相似性。实际上，给定一些维度上的城市数据，城市等级的划分其实就是城市聚类的过程。《第一财经》用商业资源集聚度、城市枢纽性、城市人活跃度、生活方式多样性和未来可塑性五大一级维度，透过主流消费品牌的商业门店数据、各领域头部互联网公司的用户行为数据和数据机构的城市大数据，衡量337 座中国地级及以上城市的商业魅力。根据《第一财经》的划分，北京、上海、广州、深圳为一线城市，成都、南京、武汉、深圳等15 个城市为新一线城市，合肥、昆明、嘉兴等30 个城市为二线城市，以及70 个三线城市和90个四线城市。 城市等级划分具有极其重要的意义。城市分级的研究对于一个城市认清自身在全国城市经济体系中的地位有参考作用。同时，准确对城市进行分级也是进行科学发展规划的前提[郭靖and 倪鹏飞,2021]。在学术上，例如在计量经济学模型中，城市等级完全可以作为一个虚拟变量加入模型，来衡量城市规模对于被解释变量（例如房价、交通等）的影响。关于城市该如何分级，学术圈和各机构都有过探讨。Friedmann 和Sassen 都认为跨国公司集中度是高等级城市的代表，因此分别采用一个城市中跨国公司集聚数量[Friedmann, 1986] 和先进生产性服务业企业集中度[Sassen, 2001] 来为城市分级；GaWC通过检验城市的经济、政治地位以及辐射带动能力，确定一座城市在世界城市网络中的位置。中国目前使用最广的城市分级主要是《第一财经》的城市商业魅力排行榜，也有学者提出自己的标准，例如使用信息、专利、知识等软功能指数以及航空、跨国公司、高端产业等硬功能指数进行城市的层次聚类[郭靖and 倪鹏飞, 2021]。然而，现实中很少有研究者在学术研究中引用城市等级标签，原因之一就是缺乏严谨计算得到的城市等级。层出不穷的城市排行提供的不同乃至自相矛盾的信息让城市管理者和社会大众无所适从，制定科学统一的“X 线城市”分级标准很有必要[张丽萍, 2020]。因此，本文提出了一种计算得到城市聚类标签的方法。我们用高斯混合模型对中国地级市进行聚类分析，划分出若干等级，每个等级内的城市符合同一个高斯分布，是出于以下考虑：（1）根据最大熵原理[Jaynes, 1957]，在精确说明先验数据的情况下，最能代表系统当前知识状态的概率分布是熵最大的概率分布。当均值和方差已知时，高斯分布的熵是所有分布中最大的；（2）现实中的很多随机变量是由大量相互独立的随机因素的综合影响所形成的，而其中每一个别因素在总的影响中所起的作用都是微小的，这种随机变量往往近似服从高斯分布。 2 数据本文所使用的数据集整理自《中国城市统计年鉴》、wind 经济数据库，以及中国各地级市的第七次人口普查公报、经济社会发展公报等。本文采集了中国290 个地级市（由于重庆市的体量相当于一个省，因此本文中重庆市的数据范围为主城九区）十个维度的数据，如表1。 夜晚城市的灯光构成了城市的点滴。夜光指数主要是利用卫星采集的图像，针对不同地区采用不同的降噪系数体系，计算出夜晚灯光的强弱来反映人类活动的频繁程度，从而推演出一个地区的经济发展情况，以及夜晚经济的繁华程度。夜光指数常被学者用来研究经济社会的发展动态，例如，有人用夜光遥感技术来研究“一带一路”沿线部分国家近20 年的经济发展规律[李德仁and 李熙, 2015]。因此，本文使用卫星采集并由佳格望眼计算的中国各城市夜光指数来作为城市的一维经济特征。 为了避免行政区划因素带来的影响（例如市辖县过多），人口密度指城区内的人口密度。人口密度=（城区人口+ 城区暂住人口）/城区面积。由于各地级市公布的GDP 都为全市级别的，为方便计算人均GDP，本文GDP、常住人口、人均GDP 采用了整个地级市层面的数据。 我们用各地级市单位GDP 的耗电量来代表当地经济绿色发展的水平。单位GDP 耗电量是某地级市全社会耗电量总量与该市生产总值的比率，是反映能源消费水平和节能降耗状况的主要指标。该指标说明一个城市经济活动的结构和能源利用的效率。单位GDP 产出的耗电量在不同城市间往往差距很大，例如，“胡焕庸线”西侧城市单位GDP 的电耗已经达到东部城市的1.8 倍左右。 教育公共预算支出是各地政府财政部门的财政预算中实际用于教育的费用支出，对教育方面的投入有助于培育更多的人力资本。而公共预算中的科学技术支出代表一个城市保障关键核心科学技术攻关的力度。本文使用各地级市政府一般公共预算支出以及其中的教育和科学技术支出，来代表该城市对人力资本培养和科学技术发展的支持力度。 3 方法3.1 高斯混合模型的定义高斯混合模型（Gaussian Mixture Model）将所有样本点分为 K 个簇，每个簇符合一个多维高斯分布 [Do, 2008]。GMM 使用这 K 个高斯分布概率密度函数的加权和作为其概率密度函数：$$\\begin{equation}p(x ; \\Theta)=\\sum_{k=1}^{K} \\alpha_{k} \\cdot \\mathcal{N}\\left(x ; \\vec{u}_{k}, \\Sigma_{k}\\right)\\end{equation}$$其中，该概率密度函数的参数$$\\begin{equation}\\Theta=\\left\\{\\alpha_{k}, \\vec{u}_{k}, \\Sigma_{k}\\right\\}_{k=1}^K\\end{equation}$$为了保证GMM的概率密度函数在区间$(-\\infty,+\\infty)$上的积分为1, 我们有$$\\begin{equation}\\int_{-\\infty}^{+\\infty} p(x ; \\Theta) d x=\\sum_{k=1}^{K}\\left[\\alpha_{k} \\int_{-\\infty}^{+\\infty} \\mathcal{N}\\left(x ; \\vec{u}_{k}, \\Sigma_{k}\\right) d x\\right]=\\sum_{k=1}^{K} \\alpha_{k}=1\\end{equation}$$若我们很清楚地知道整个数据集的采样过程，即我们知道样本$x_i$是从第$z_i$个高斯分布簇采样得到，那我们可以通过构造NLL函数很轻松得到各个簇的参数值。但事实上，我们并不清楚各个高斯分布簇的分布情况以及样本点的采样过程，上述参数估计过程无法实现。因此我们采用EM算法进行参数估计。 3.2 EM算法在GMM模型的实现在《数学之美》这本书中，EM 算法被誉为“上帝的算法”。EM 算法延续了 MLE 的思路, 通过不断地构造对数似然函数的下界函数, 并对这个较为容易求解的下界函数进行最优化, 以增大对数似然函数取值的下界, 使得在不断的迭代操作后, 对数似然函数的取值能逼近最大值, 从而完成参数估计 [Dempster et al., 1977]。$$\\begin{equation}\\begin{aligned}\\mathcal{L} \\mathcal{L}(\\Theta) &amp;=\\sum_{i=1}^{n} \\log \\left[\\sum_{z_{i}} p\\left(x_{i}, z_{i} ; \\Theta\\right)\\right] \\\\&amp;=\\sum_{i=1}^{n} \\log \\left[\\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right]\\end{aligned}\\end{equation}$$其中，$p\\left(z_{i} \\mid x_{i}; \\Theta_{t-1}\\right)$为给定第$t-1$次迭代后$z_{i}$关于$x_{i}$的后验分布。 $$\\begin{equation}p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)=\\frac{p\\left(x_{i} \\mid z_{i} ; \\Theta_{t-1}\\right) \\cdot p\\left(z_{i} ; \\Theta_{t-1}\\right)}{\\sum_{z_{i}} p\\left(x_{i} \\mid z_{i} ; \\Theta_{t-1}\\right) \\cdot p\\left(z_{i} ; \\Theta_{t-1}\\right)}\\end{equation}$$使用这个关于$z_{i}$的后验分布构造期望$$\\begin{equation}\\mathbf{E}_{z_{i}}\\left[\\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right]=\\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\end{equation}$$由琴生不等式 [McShane, 1937]，$$\\begin{equation}\\log \\mathbf{E}_{z_{i}}\\left[\\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right] \\geq \\mathbf{E}_{z_{i}}\\left[\\log \\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right]\\end{equation}$$即$$\\begin{equation}\\log \\left[\\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right] \\geq \\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\log \\left[\\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right]\\end{equation}$$因此，对数似然函数$$\\begin{equation}\\begin{aligned}\\mathcal{L} \\mathcal{L}(\\Theta) &amp;=\\sum_{i=1}^{n} \\log \\left[\\sum_{z_{i}} p\\left(x_{i}, z_{i} ; \\Theta\\right)\\right] \\\\&amp;=\\sum_{i=1}^{n} \\log \\left[\\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right] \\\\&amp; \\geq \\sum_{i=1}^{n} \\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\log \\left[\\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right]\\end{aligned}\\end{equation}$$我们定义下界函数，作为EM算法中的Expectation$$\\begin{equation}\\begin{aligned}\\mathcal{Q}\\left(\\Theta, \\Theta_{t-1}\\right)&amp;=\\sum_{i=1}^{n} \\sum_{z_{i}} p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right) \\cdot \\log \\left[\\frac{p\\left(x_{i}, z_{i} ; \\Theta\\right)}{p\\left(z_{i} \\mid x_{i} ; \\Theta_{t-1}\\right)}\\right]\\end{aligned}\\end{equation}$$因此$$\\begin{equation}\\mathcal{L} \\mathcal{L}(\\Theta) \\geq \\mathcal{Q}\\left(\\Theta, \\Theta_{t-1}\\right)\\end{equation}$$我们将最优化下界函数得到的$\\Theta$作为$\\Theta_{t}$,这一步也被称作EM算法中的Maximum$$\\begin{equation}\\Theta_{t}=\\underset{\\Theta}{\\operatorname{argmax}} \\mathcal{Q}\\left(\\Theta, \\Theta_{t-1}\\right)\\end{equation}$$重复以上E-M步骤，直到达到我们指定的迭代次数或者$\\Theta_{t}$不再变化。可以证明，对数似然函数满足收敛性，即$\\mathcal{L L}\\left(\\Theta_{t}\\right) \\geq \\mathcal{L L}\\left(\\Theta_{t-1}\\right), t \\in[1,+\\infty)$，证明过程本文略。 3.3 代入多维高斯模型的参数对于每个高斯分布簇，$\\Theta_{t-1}=\\left\\{\\alpha_{k}^{t-1}, \\vec{u}_{k}^{t-1}, \\Sigma_{k}^{t-1}\\right\\}_{k=1}^{K}$，代入各参数，求得下界函数为$$\\begin{equation}\\begin{aligned}\\mathcal{Q}\\left(\\Theta, \\Theta_{t-1}\\right) &amp;=\\sum_{i=1}^{n} \\sum_{k=1}^{K} q_{i k}\\left[-\\frac{n}{2} \\log 2 \\pi-\\frac{1}{2} \\log \\left|\\Sigma_{k}\\right|-\\frac{1}{2} \\cdot\\left[\\left(x_{i}-\\vec{\\mu}_{k}\\right)^{\\top} \\Sigma_{k}^{-1}\\left(x_{i}-\\vec{\\mu}_{k}\\right)\\right]+\\log \\alpha_{k}-\\log q_{i k}\\right] \\\\&amp;=\\sum_{i=1}^{n} \\sum_{k=1}^{K} q_{i k}\\left[-\\frac{1}{2} \\log \\left|\\Sigma_{k}\\right|-\\frac{1}{2} \\cdot\\left[\\left(x_{i}-\\vec{\\mu}_{k}\\right)^{\\top} \\Sigma_{k}^{-1}\\left(x_{i}-\\vec{\\mu}_{k}\\right)\\right]+\\log \\alpha_{k}\\right]\\end{aligned}\\end{equation}$$下界函数对参数求偏导，最终得到各参数迭代后的取值 4 实验与分析4.1 最优簇个数的确定我们使用 Calinski-Harabaz 指数 [Kwon et al., 2017] 来确定最优的高斯分布的簇数，Calinski-Harabaz指数定义为$$\\begin{equation}s(k)=\\frac{\\operatorname{Tr}\\left(B_{k}\\right)}{\\operatorname{Tr}\\left(W_{k}\\right)} \\times \\frac{N-k}{k-1}\\end{equation}$$其中$W_{k}=\\sum_{q=1}^{k} \\sum_{x \\in C_{q}}\\left(x-c_{q}\\right)\\left(x-c_{q}\\right)^{T}$为簇内分散矩阵，$B_{k}=\\sum_{q} n_{q}\\left(c_{q}-c\\right)\\left(c_{q}-c\\right)^{T}$为簇间分散矩阵，$c_{q}$为第q个簇的中心点，$c$为整个样本集的中心点。Calinski-Harabaz指数越大，代表各类之间联系越紧密而不同类之间越分散，则聚类效果越好。 通过指定簇数为2-6,用我们的城市样本数据集计算了不同簇个数对应的Calinski-Harabaz指数，计算结果如图1。根据结果，我们最终确定最优簇个数为5，即中国城市被分为5个等级。 4.2 中国城市聚类结果我们已经通过计算得出中国城市聚类的最优簇数，给定最优簇数5，我们对中国290个地级市的十维数据进行高斯混合聚类，聚类结果如图2。 根据聚类结果，我们可以给出我们模型得到的中国城市等级，如表2。 5 讨论 根据高斯混合模型以及CH指数，我们得出中国的地级市大体上可以分为五个簇，每个簇符合一个高斯分布。我们将实验结果与媒体所评选出的一二三线商业城市进行对比，发现结果相似，证明高斯混合聚类在中国城市等级划分上具有良好的效果。和媒体的城市分级结果不同，我们的分级未指定每个簇应该有的城市个数，数据集也不仅仅集中在商业领域，因此我们的结果会与媒体评出的有差异。但由于我们的结果是由计算机根据数学模型计算得到，因此会有较好的后续学术应用性。 由于《中国城市统计年鉴》上的数据多为整个地级市行政区划内的数据，而并非自然生长的城区数据，因此，数据集可能受行政区划以及区划调整影响过大。例如，虽然人口密度等维度我们采用的是城区数据，但GDP、公共预算支出却很难做到以城区为口径。因此，国家统计局城市司建议使用“市辖区”作为统计口径[张丽萍, 2020]以剔除各城市市辖县数目差异带来的偏差，剔除我国城市的非城市因素，若可以获得市辖区级别的各地级市数据，聚类结果会更加贴合实际情况。 需要强调的是，本文旨在提出城市分级的一种可能的方法，而不代表最终分级的结果。事实上，受制于数据的可获得性，本文仅采用了城市十个维度的数据，并且主要是宏观数据。高斯混合模型往往在数据维度更高时会获得更准确的分类结果。因此，在实际操作时，可以考虑在信息、知识、资本、外资吸引力、机场港口、交通系统与通信设备等方面增加更多维度的城市数据。同时不仅关注宏观，也要聚焦微观个体，例如一线品牌、跨国公司的数量和密度。 本文所采用数据基本是城市的静态数据，而未从“联系”的视角来考察中国城市。事实上，由于经贸人员往来，中国城市已成为一个相互联系的网络体系。同时随着经济全球化的发展，国际分工的明确，中国城市与国外城市的连接也越来越复杂。如何利用好例如高铁、航班、人口流、资金流等“关系型数据”，从基于城市联系的角度将网络分析与本文方法结合起来，是后续值得改进的方向。 正如前所述，城市等级划分意义深远。学术上来说，城市等级不仅仅可以作为计量经济学模型中的一个变量，在后续的研究中，还可以考虑依据我们聚类得到的标签，在一个城市群（City cluster）内部研究拓扑结构（例如卫星城效应、城市群混沌程度等），前景广阔。实际来看，城市分级对于厘清城市之间的差异和联系，乃至各城市制定科学的发展战略，都有重要的意义。 参考文献Jordan Rappaport et al. A guide to aggregate house price measures. Economic Review-Federal Reserve Bank of Kansas City, 92(2):41, 2007. 郭靖and 倪鹏飞. 新视角下全球城市分级的理论依据与实践启示. 区域经济评论, 2021. John Friedmann. The world city hypothesis. Development and Change, 17(1):69–83, 1986. S Sassen. The global city: New york, london, tokyo. princeton, oxford: Princeton university press. 2001. 张丽萍. “x 线城市”——中国城市分级方法探讨. 中国统计, 2020. Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957. 李德仁and 李熙. 夜光遥感技术在评估经济社会发展中的应用——兼论其对“一带一路”建设质量的保障. 宏观质量研究, 3(4):1–8, 2015. Chuong B Do. More on multivariate gaussians. URL http://cs229. stanford. edu/section/more_on_gaussians. pdf.[Online, 2008. Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1–22, 1977. Edward James McShane. Jensen’s inequality. Bulletin of the American Mathematical Society, 43(8):521–527, 1937. Bum Chul Kwon, Ben Eysenbach, Janu Verma, Kenney Ng, Christopher De Filippi, Walter F Stewart, and Adam Perer. Clustervision: Visual supervision of unsupervised clustering. IEEE transactions on visualization and computer graphics, 24(1):142–151, 2017.","link":"/2022/05/21/GMMCityClustering/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Causal Inference","slug":"Causal-Inference","link":"/tags/Causal-Inference/"},{"name":"Reading Notes","slug":"Reading-Notes","link":"/tags/Reading-Notes/"},{"name":"Mathematics","slug":"Mathematics","link":"/tags/Mathematics/"},{"name":"Research","slug":"Research","link":"/tags/Research/"},{"name":"Quant","slug":"Quant","link":"/tags/Quant/"},{"name":"Urban Governance","slug":"Urban-Governance","link":"/tags/Urban-Governance/"}],"categories":[{"name":"Reading Notes","slug":"Reading-Notes","link":"/categories/Reading-Notes/"},{"name":"Research","slug":"Research","link":"/categories/Research/"},{"name":"Machine Learning","slug":"Reading-Notes/Machine-Learning","link":"/categories/Reading-Notes/Machine-Learning/"},{"name":"Mathematics","slug":"Research/Mathematics","link":"/categories/Research/Mathematics/"},{"name":"Urban Governance","slug":"Research/Urban-Governance","link":"/categories/Research/Urban-Governance/"}]}